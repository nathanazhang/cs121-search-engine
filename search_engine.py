"""
This code was mainly generated by AI
and then manually edited to fit assignment specifications.
"""

import json
import math
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple

from config import (
    DOC_META_PATH,
    LEXICON_PATH,
    TITLE_WEIGHT,
    HEADING_WEIGHT,
    BOLD_WEIGHT,
    ANCHOR_WEIGHT,
    BASE_TF_WEIGHT,
    PAGERANK_WEIGHT,
    NGRAM_BOOST,
    PROXIMITY_BOOST,
    MAX_RESULTS,
    WEB_HOST,
    WEB_PORT,
    DEBUG_MODE,
)
from utils import tokenize, stem, build_ngrams, log_idf

from analytics_store import record_query_time
import time

# ----------------- LOAD METADATA & LEXICON -----------------

doc_meta: Dict[int, dict] = {}
lexicon: Dict[str, dict] = {}

def load_metadata():
    global doc_meta, lexicon
    doc_meta = json.loads(Path(DOC_META_PATH).read_text(encoding="utf-8"))
    lexicon = json.loads(Path(LEXICON_PATH).read_text(encoding="utf-8"))

# ----------------- POSTINGS ACCESS -----------------

def read_postings(term: str) -> Tuple[Dict[int, dict], str]:
    """
    Read postings for a term from the appropriate index file.
    Returns (postings_dict, term_type).
    Includes safety checks for malformed lines.
    """
    entry = lexicon.get(term)
    if not entry:
        return {}, ""

    file_path = Path(entry["file"])
    offset = entry["offset"]
    term_type = entry["type"]

    with file_path.open("rb") as f:
        f.seek(offset)
        line = f.readline().decode("utf-8", errors="ignore")

    # SAFETY FIX: skip malformed lines
    parts = line.split("\t", 1)
    if len(parts) != 2:
        return {}, ""

    _term, rest = parts
    try:
        postings = json.loads(rest.strip())
    except Exception:
        return {}, ""

    postings_int = {int(k): v for k, v in postings.items()}
    return postings_int, term_type

# ----------------- RANKING -----------------

def compute_scores(query: str) -> List[Tuple[int, float]]:
    q_tokens = tokenize(query)
    q_stems = [stem(t) for t in q_tokens]

    q_tf: Dict[str, int] = defaultdict(int)
    for t in q_stems:
        q_tf[t] += 1

    q_bigrams = build_ngrams(q_stems, 2)
    q_trigrams = build_ngrams(q_stems, 3)

    scores: Dict[int, float] = defaultdict(float)
    N = len(doc_meta)

    # 1) Unigram tf-idf
    for term, qfreq in q_tf.items():
        postings, term_type = read_postings(term)
        if not postings or term_type != "unigram":
            continue

        df = len(postings)
        idf = log_idf(N, df)
        wq = (1 + math.log(qfreq)) * idf

        for doc_id, pdata in postings.items():
            meta = doc_meta.get(str(doc_id))
            if not meta or meta.get("duplicate_of") is not None:
                continue

            tf = pdata.get("tf", 0)
            if tf <= 0:
                continue

            wd = (1 + math.log(tf))

            if pdata.get("title"):
                wd *= TITLE_WEIGHT
            if pdata.get("heading"):
                wd *= HEADING_WEIGHT
            if pdata.get("bold"):
                wd *= BOLD_WEIGHT
            if pdata.get("anchor"):
                wd *= ANCHOR_WEIGHT

            scores[doc_id] += BASE_TF_WEIGHT * wq * wd

    # 2) N-gram boosts
    for ngram in q_bigrams + q_trigrams:
        postings, _ = read_postings(ngram)
        if not postings:
            continue

        df = len(postings)
        idf = log_idf(N, df)

        for doc_id, pdata in postings.items():
            tf = pdata.get("tf", 0)
            if tf > 0:
                scores[doc_id] += NGRAM_BOOST * (1 + math.log(tf)) * idf

    # 3) Positional proximity
    if len(q_tf) > 1:
        doc_term_positions = defaultdict(lambda: defaultdict(list))

        for term in q_tf.keys():
            postings, term_type = read_postings(term)
            if not postings or term_type != "unigram":
                continue

            for doc_id, pdata in postings.items():
                positions = pdata.get("positions", [])
                if positions:
                    doc_term_positions[doc_id][term].extend(positions)

        for doc_id, term_pos in doc_term_positions.items():
            if len(term_pos) < 2:
                continue

            all_positions = []
            for term, pos_list in term_pos.items():
                for p in pos_list:
                    all_positions.append((p, term))

            all_positions.sort()

            best_span = None
            left = 0
            term_counts = defaultdict(int)
            distinct_terms = 0

            for right in range(len(all_positions)):
                pos_r, term_r = all_positions[right]
                if term_counts[term_r] == 0:
                    distinct_terms += 1
                term_counts[term_r] += 1

                while distinct_terms >= 2 and left <= right:
                    pos_l, term_l = all_positions[left]
                    span = pos_r - pos_l + 1
                    if best_span is None or span < best_span:
                        best_span = span

                    term_counts[term_l] -= 1
                    if term_counts[term_l] == 0:
                        distinct_terms -= 1
                    left += 1

            if best_span is not None and best_span <= 10:
                scores[doc_id] *= PROXIMITY_BOOST

    # 4) PageRank
    for doc_id in list(scores.keys()):
        meta = doc_meta.get(str(doc_id), {})
        pr = meta.get("pagerank", 0.0)
        scores[doc_id] += PAGERANK_WEIGHT * pr

    # Normalize by doc length
    for doc_id in list(scores.keys()):
        length = doc_meta.get(str(doc_id), {}).get("length", 1)
        if length > 0:
            scores[doc_id] /= math.sqrt(length)

    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return ranked[:MAX_RESULTS]

# ----------------- CLI INTERFACE -----------------

def cli_loop():
    load_metadata()
    print("CLI Search Engine. Type 'quit' to exit.")
    while True:
        q = input("Query> ").strip()
        if q.lower() == "quit":
            break

        # Analytics [Begin]
        start = time.perf_counter()
        results = compute_scores(q)
        elapsed_ms = (time.perf_counter() - start) * 1000
        record_query_time(elapsed_ms)
        print(f"Query time: {elapsed_ms:.2f} ms")
        # Analytics [End]

        print(f"Top {len(results)} results:")
        for rank, (doc_id, score) in enumerate(results, start=1):
            meta = doc_meta[str(doc_id)]
            print(f"{rank}. {meta['url']} (score={score:.4f})")

if __name__ == "__main__":
    import sys
    load_metadata()
    cli_loop()
