"""
This code was mainly generated by AI
and then manually edited to fit assignment specifications.
"""

import os
from pathlib import Path

# ----------------- GLOBAL CONFIGURATION -----------------

# **Path to the DEV corpus root folder** (each subfolder is a subdomain).
DEV_ROOT = Path("DEV_Test")  # change if needed

# **Path where all index files and metadata will be stored.**
INDEX_ROOT = Path("index_data")
INDEX_ROOT.mkdir(parents=True, exist_ok=True)

# **Partial index directory** (for spill files during indexing).
PARTIAL_INDEX_DIR = INDEX_ROOT / "partials"
PARTIAL_INDEX_DIR.mkdir(parents=True, exist_ok=True)

# **Final index files directory.**
FINAL_INDEX_DIR = INDEX_ROOT / "final"
FINAL_INDEX_DIR.mkdir(parents=True, exist_ok=True)

# **Metadata files.**
DOC_META_PATH = INDEX_ROOT / "doc_meta.json"          # doc_id -> metadata (url, length, pagerank, etc.)
LEXICON_PATH = INDEX_ROOT / "lexicon.json"            # term -> { "file": ..., "offset": ..., "df": ..., "type": "unigram/bigram/trigram" }
DUPLICATE_MAP_PATH = INDEX_ROOT / "duplicates.json"   # duplicate/near-duplicate mapping

# **Index file names (we'll keep unigrams, bigrams, trigrams separate).**
UNIGRAM_INDEX_PATH = FINAL_INDEX_DIR / "unigram_index.txt"
BIGRAM_INDEX_PATH = FINAL_INDEX_DIR / "bigram_index.txt"
TRIGRAM_INDEX_PATH = FINAL_INDEX_DIR / "trigram_index.txt"

# **Spill threshold: when in-memory postings exceed this many distinct terms, flush to disk.**
MAX_TERMS_IN_MEMORY = 100_000  # tune as needed

# **Near-duplicate detection parameters.**
SHINGLE_SIZE = 5               # number of tokens per shingle
NEAR_DUP_JACCARD_THRESHOLD = 0.9

# **PageRank parameters.**
PAGERANK_DAMPING = 0.85
PAGERANK_ITERATIONS = 20

# **Ranking weights.**
TITLE_WEIGHT = 3.0
HEADING_WEIGHT = 2.0
BOLD_WEIGHT = 1.5
ANCHOR_WEIGHT = 2.5
BASE_TF_WEIGHT = 1.0
PAGERANK_WEIGHT = 0.3          # how much PageRank influences final score
NGRAM_BOOST = 1.2              # multiplier for docs matching bigrams/trigrams
PROXIMITY_BOOST = 1.3          # multiplier for close-term proximity

# **Search constraints.**
MAX_RESULTS = 50               # max docs to return
RESPONSE_TIME_TARGET_MS = 300  # for your own sanity checks, not enforced in code

# **Web interface config.**
WEB_HOST = "127.0.0.1"
WEB_PORT = 5000
DEBUG_MODE = False

# ----------------- UTILITY -----------------

def ensure_dirs():
    """Ensure all required directories exist."""
    for p in [INDEX_ROOT, PARTIAL_INDEX_DIR, FINAL_INDEX_DIR]:
        os.makedirs(p, exist_ok=True)
