"""
This code was mainly generated by AI
and then manually edited to fit assignment specifications.
"""

import json
import math
import os
import re
from pathlib import Path
from typing import List, Dict, Tuple, Set

from bs4 import BeautifulSoup  # pip install beautifulsoup4
from nltk.stem import PorterStemmer  # pip install nltk

ps = PorterStemmer()

# Simple alphanumeric tokenizer regex.
TOKEN_RE = re.compile(r"[A-Za-z0-9]+")

def load_json(path: Path):
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

def save_json(path: Path, obj):
    with path.open("w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2)

def tokenize(text: str) -> List[str]:
    """Tokenize into alphanumeric tokens, lowercase, no stemming yet."""
    return [t.lower() for t in TOKEN_RE.findall(text)]

def stem(token: str) -> str:
    """Apply Porter stemming."""
    return ps.stem(token)

def extract_html_fields(html: str):
    """
    Parse HTML and extract:
    - full text tokens
    - title tokens
    - heading tokens (h1-h3)
    - bold tokens
    - anchor tokens + link targets
    """
    soup = BeautifulSoup(html, "lxml")  # lxml handles broken HTML reasonably well

    # Title
    title_text = soup.title.get_text(separator=" ", strip=True) if soup.title else ""
    title_tokens = tokenize(title_text)

    # Headings
    heading_tokens = []
    for tag_name in ["h1", "h2", "h3"]:
        for tag in soup.find_all(tag_name):
            heading_tokens.extend(tokenize(tag.get_text(separator=" ", strip=True)))

    # Bold/strong
    bold_tokens = []
    for tag in soup.find_all(["b", "strong"]):
        bold_tokens.extend(tokenize(tag.get_text(separator=" ", strip=True)))

    # Anchor text + hrefs
    anchor_tokens = []
    anchor_links = []  # list of (href, anchor_text_tokens)
    for a in soup.find_all("a"):
        href = a.get("href")
        if not href:
            continue
        text = a.get_text(separator=" ", strip=True)
        tokens = tokenize(text)
        if tokens:
            anchor_tokens.extend(tokens)
            anchor_links.append((href, tokens))

    # Full visible text (fallback: soup.get_text)
    full_text = soup.get_text(separator=" ", strip=True)
    full_tokens = tokenize(full_text)

    return {
        "full_tokens": full_tokens,
        "title_tokens": title_tokens,
        "heading_tokens": heading_tokens,
        "bold_tokens": bold_tokens,
        "anchor_tokens": anchor_tokens,
        "anchor_links": anchor_links,
    }

def build_ngrams(tokens: List[str], n: int) -> List[str]:
    """Build n-grams as 'token1_token2_..._tokenn' strings."""
    if len(tokens) < n:
        return []
    return ["_".join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]

def compute_shingles(tokens: List[str], k: int) -> Set[str]:
    """Compute k-shingles (for near-duplicate detection)."""
    return set(build_ngrams(tokens, k))

def jaccard(a: Set[str], b: Set[str]) -> float:
    if not a and not b:
        return 1.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0

def log_idf(N: int, df: int) -> float:
    """Compute log-based idf; add 1 to avoid division by zero."""
    return math.log((N + 1) / (df + 1)) + 1.0
