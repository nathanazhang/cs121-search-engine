"""
This code was mainly generated by AI
and then manually edited to fit assignment specifications.
"""

import json
import os
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Set, Tuple

from config import (
    DEV_ROOT,
    PARTIAL_INDEX_DIR,
    FINAL_INDEX_DIR,
    UNIGRAM_INDEX_PATH,
    BIGRAM_INDEX_PATH,
    TRIGRAM_INDEX_PATH,
    DOC_META_PATH,
    LEXICON_PATH,
    DUPLICATE_MAP_PATH,
    MAX_TERMS_IN_MEMORY,
    SHINGLE_SIZE,
    NEAR_DUP_JACCARD_THRESHOLD,
    PAGERANK_DAMPING,
    PAGERANK_ITERATIONS,
    ensure_dirs,
)

from utils import (
    load_json,
    save_json,
    extract_html_fields,
    stem,
    build_ngrams,
    compute_shingles,
    jaccard,
)


# ============================================================
#  Utility: Walk the DEV corpus
# ============================================================

def walk_corpus(root: Path):
    """Yield every JSON file in the DEV dataset."""
    for subdir, _, files in os.walk(root):
        for fname in files:
            if fname.endswith(".json"):
                yield Path(subdir) / fname


# ============================================================
#  Duplicate Detection (Exact + Near)
# ============================================================

def detect_duplicates(
    doc_id: int,
    tokens: List[str],
    shingle_index: Dict[str, List[int]],
    doc_shingles: Dict[int, Set[str]],
    duplicate_map: Dict[int, int],
) -> bool:
    """
    Detect exact and near-duplicate documents.
    Returns True if this doc is a duplicate and should be skipped.
    """

    # ---------- Exact duplicate ----------
    token_str = " ".join(tokens)
    token_hash = hash(token_str)
    exact_key = f"exact:{token_hash}"

    if exact_key in duplicate_map:
        canonical = duplicate_map[exact_key]
        duplicate_map[doc_id] = canonical
        return True
    else:
        duplicate_map[exact_key] = doc_id

    # ---------- Near-duplicate ----------
    shingles = compute_shingles(tokens, SHINGLE_SIZE)
    doc_shingles[doc_id] = shingles

    candidate_docs = set()
    for sh in shingles:
        for other in shingle_index.get(sh, []):
            candidate_docs.add(other)

    for other in candidate_docs:
        sim = jaccard(shingles, doc_shingles[other])
        if sim >= NEAR_DUP_JACCARD_THRESHOLD:
            duplicate_map[doc_id] = other
            return True

    # Update shingle index
    for sh in shingles:
        shingle_index.setdefault(sh, []).append(doc_id)

    return False


# ============================================================
#  PageRank Graph Construction
# ============================================================

def add_links_to_graph(
    source_id: int,
    source_url: str,
    anchor_links,
    url_to_id: Dict[str, int],
    out_links: Dict[int, Set[int]],
):
    """
    Build link graph from anchor tags.
    Only internal ICS links are considered.
    """
    for href, _tokens in anchor_links:
        if not href:
            continue

        # Normalize URL
        if href.startswith("http"):
            if "ics.uci.edu" not in href:
                continue
            target_url = href.split("#")[0]
        else:
            # Relative URL â†’ skip complex normalization
            continue

        if target_url not in url_to_id:
            continue

        target_id = url_to_id[target_url]
        out_links.setdefault(source_id, set()).add(target_id)


# ============================================================
#  Partial Index Spill
# ============================================================

def write_partial_index(
    part_id: int,
    uni: Dict[str, Dict[int, dict]],
    bi: Dict[str, Dict[int, dict]],
    tri: Dict[str, Dict[int, dict]],
) -> Tuple[Path, Path, Path]:

    uni_path = PARTIAL_INDEX_DIR / f"unigram_part_{part_id}.txt"
    bi_path = PARTIAL_INDEX_DIR / f"bigram_part_{part_id}.txt"
    tri_path = PARTIAL_INDEX_DIR / f"trigram_part_{part_id}.txt"

    def write(path: Path, postings: Dict[str, Dict[int, dict]]):
        with path.open("w", encoding="utf-8") as f:
            for term in sorted(postings.keys()):
                line = json.dumps(postings[term], separators=(",", ":"))
                f.write(f"{term}\t{line}\n")

    write(uni_path, uni)
    write(bi_path, bi)
    write(tri_path, tri)

    return uni_path, bi_path, tri_path


# ============================================================
#  Merge Partial Index Files (Rewritten, Robust)
# ============================================================

def merge_partial_files(
    partial_paths: List[Path],
    final_path: Path,
    lexicon: Dict[str, dict],
    term_type: str,
):
    """
    Robust multi-way merge:
    - Reads partial files in text mode
    - Writes final index in binary mode
    - Computes correct byte offsets
    - Skips malformed lines
    """

    # Open partial files in TEXT mode
    files = [p.open("r", encoding="utf-8") for p in partial_paths]
    buffers = [f.readline() for f in files]

    # Open final index in BINARY mode
    with final_path.open("wb") as out:
        offset = 0

        while True:
            # Collect current terms
            candidates = []
            for i, line in enumerate(buffers):
                if not line or "\t" not in line:
                    continue
                term = line.split("\t", 1)[0]
                candidates.append((term, i))

            if not candidates:
                break

            candidates.sort(key=lambda x: x[0])
            smallest_term = candidates[0][0]

            merged = {}

            # Merge postings for smallest_term
            for i, line in enumerate(buffers):
                if not line or "\t" not in line:
                    continue

                term, rest = line.split("\t", 1)
                if term != smallest_term:
                    continue

                try:
                    postings = json.loads(rest.strip())
                except Exception:
                    buffers[i] = files[i].readline()
                    continue

                for doc_id_str, pdata in postings.items():
                    doc_id = int(doc_id_str)
                    if doc_id not in merged:
                        merged[doc_id] = pdata
                    else:
                        # Merge fields
                        for k, v in pdata.items():
                            if k == "positions":
                                merged[doc_id].setdefault("positions", [])
                                merged[doc_id]["positions"].extend(v)
                            elif k == "tf":
                                merged[doc_id]["tf"] = merged[doc_id].get("tf", 0) + v
                            else:
                                merged[doc_id][k] = merged[doc_id].get(k, False) or v

                buffers[i] = files[i].readline()

            # Write merged line in BINARY mode
            line_str = json.dumps(merged, separators=(",", ":"))
            out_line = f"{smallest_term}\t{line_str}\n".encode("utf-8")

            out.write(out_line)

            # Record lexicon entry
            lexicon[smallest_term] = {
                "file": str(final_path),
                "offset": offset,
                "df": len(merged),
                "type": term_type,
            }

            offset += len(out_line)

    for f in files:
        f.close()


# ============================================================
#  PageRank Computation
# ============================================================

def compute_pagerank(num_docs: int, out_links: Dict[int, Set[int]]) -> Dict[int, float]:
    N = num_docs
    pr = {i: 1.0 / N for i in range(N)}

    in_links = {i: set() for i in range(N)}
    for src, targets in out_links.items():
        for t in targets:
            in_links[t].add(src)

    for _ in range(PAGERANK_ITERATIONS):
        new_pr = {}
        for i in range(N):
            rank_sum = 0.0
            for j in in_links[i]:
                outdeg = len(out_links.get(j, []))
                if outdeg > 0:
                    rank_sum += pr[j] / outdeg
            new_pr[i] = (1 - PAGERANK_DAMPING) / N + PAGERANK_DAMPING * rank_sum
        pr = new_pr

    return pr


# ============================================================
#  MAIN INDEXER
# ============================================================

def build_index():
    ensure_dirs()

    unigram = {}
    bigram = {}
    trigram = {}

    doc_meta = {}
    duplicate_map = {}
    shingle_index = {}
    doc_shingles = {}

    url_to_id = {}
    out_links = {}

    partial_uni = []
    partial_bi = []
    partial_tri = []

    current_doc_id = 0
    part_id = 0

    for page_path in walk_corpus(DEV_ROOT):
        url, html = load_json(page_path).get("url", ""), load_json(page_path).get("content", "")
        if not url:
            continue

        doc_id = current_doc_id
        current_doc_id += 1
        url_to_id[url] = doc_id

        fields = extract_html_fields(html)
        full_tokens = fields["full_tokens"]

        # Duplicate detection
        if detect_duplicates(doc_id, full_tokens, shingle_index, doc_shingles, duplicate_map):
            doc_meta[doc_id] = {"url": url, "duplicate_of": duplicate_map[doc_id], "length": 0}
            continue

        # Stem tokens
        stems = [stem(t) for t in full_tokens]
        title_stems = set(stem(t) for t in fields["title_tokens"])
        heading_stems = set(stem(t) for t in fields["heading_tokens"])
        bold_stems = set(stem(t) for t in fields["bold_tokens"])
        anchor_stems = set(stem(t) for t in fields["anchor_tokens"])

        # Unigrams with positions
        term_positions = defaultdict(list)
        for pos, tok in enumerate(stems):
            term_positions[tok].append(pos)

        for term, positions in term_positions.items():
            entry = unigram.setdefault(term, {}).setdefault(doc_id, {
                "tf": 0,
                "positions": [],
                "title": False,
                "heading": False,
                "bold": False,
                "anchor": False,
            })
            entry["tf"] += len(positions)
            entry["positions"].extend(positions)
            entry["title"] = entry["title"] or (term in title_stems)
            entry["heading"] = entry["heading"] or (term in heading_stems)
            entry["bold"] = entry["bold"] or (term in bold_stems)
            entry["anchor"] = entry["anchor"] or (term in anchor_stems)

        # N-grams
        for bg in build_ngrams(stems, 2):
            bigram.setdefault(bg, {}).setdefault(doc_id, {"tf": 0})
            bigram[bg][doc_id]["tf"] += 1

        for tg in build_ngrams(stems, 3):
            trigram.setdefault(tg, {}).setdefault(doc_id, {"tf": 0})
            trigram[tg][doc_id]["tf"] += 1

        # PageRank graph
        add_links_to_graph(doc_id, url, fields["anchor_links"], url_to_id, out_links)

        # Metadata
        doc_meta[doc_id] = {"url": url, "duplicate_of": None, "length": len(stems)}

        # Spill if needed
        if len(unigram) > MAX_TERMS_IN_MEMORY:
            part_id += 1
            u, b, t = write_partial_index(part_id, unigram, bigram, trigram)
            partial_uni.append(u)
            partial_bi.append(b)
            partial_tri.append(t)
            unigram.clear()
            bigram.clear()
            trigram.clear()

    # Final spill
    if unigram:
        part_id += 1
        u, b, t = write_partial_index(part_id, unigram, bigram, trigram)
        partial_uni.append(u)
        partial_bi.append(b)
        partial_tri.append(t)

    # Merge partials
    lexicon = {}
    merge_partial_files(partial_uni, UNIGRAM_INDEX_PATH, lexicon, "unigram")
    merge_partial_files(partial_bi, BIGRAM_INDEX_PATH, lexicon, "bigram")
    merge_partial_files(partial_tri, TRIGRAM_INDEX_PATH, lexicon, "trigram")

    # PageRank
    pr = compute_pagerank(current_doc_id, out_links)
    for doc_id in doc_meta:
        doc_meta[doc_id]["pagerank"] = pr.get(doc_id, 0.0)

    # Save metadata
    save_json(DOC_META_PATH, doc_meta)
    save_json(LEXICON_PATH, lexicon)
    save_json(DUPLICATE_MAP_PATH, duplicate_map)

    print("Indexing complete.")
    print(f"Documents processed: {current_doc_id}")
    print(f"Final index files written to: {FINAL_INDEX_DIR}")


if __name__ == "__main__":
    build_index()
